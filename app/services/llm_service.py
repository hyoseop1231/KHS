import requests
import aiohttp
import asyncio
import json
from typing import List, Dict, Any, Optional, Generator
from app.config import settings
from app.utils.logging_config import get_logger
from app.utils.exceptions import LLMError

logger = get_logger(__name__)

def format_cached_response_with_llm(cached_text: str, model_name: str = None) -> str:
    """
    Format cached response text using LLM to add proper markdown structure
    """
    if not model_name:
        model_name = settings.OLLAMA_DEFAULT_MODEL
    
    # Create a formatting prompt
    formatting_prompt = f"""ë‹¤ìŒ ì£¼ì¡° ê¸°ìˆ  í…ìŠ¤íŠ¸ë¥¼ ì½ê¸° ì‰½ê²Œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”. 
ë‚´ìš©ê³¼ ì „ë¬¸ ìš©ì–´ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê³ , ì˜¤ì§ í˜•ì‹ë§Œ ê°œì„ í•´ì£¼ì„¸ìš”:

**í˜•ì‹ ê°œì„  ê·œì¹™:**
- ì œëª©ê³¼ ì†Œì œëª©ì— # ## ### ì‚¬ìš©
- ì¤‘ìš”í•œ ë‚´ìš©ì€ **êµµê²Œ** í‘œì‹œ
- ëª©ë¡ì€ - ë˜ëŠ” 1. ì‚¬ìš©
- ì£¼ì¡° ì „ë¬¸ìš©ì–´ëŠ” `ë°±í‹±` ì‚¬ìš© (ì˜ˆ: `ì£¼í˜•`, `íƒ•êµ¬`, `ë¼ì´ì €`)
- í‘œê°€ ìˆë‹¤ë©´ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬
- ê¸´ ë¬¸ë‹¨ì€ ì ì ˆíˆ ë‚˜ëˆ„ê¸°
- ê¸°ìˆ ì  ìˆ˜ì¹˜ë‚˜ ë°ì´í„°ëŠ” ëª…í™•íˆ êµ¬ë¶„

**ì£¼ì¡° ì „ë¬¸ ìš©ì–´ ìœ ì§€:**
ì£¼í˜•, íƒ•êµ¬, ë¼ì´ì €, ì½”ì–´, íŒ¨í„´, ì‚¬í˜•, ê¸ˆí˜•, ê²°í•¨, ìˆ˜ì¶•, ì‘ê³ , ìš©í•´ ë“±ì˜ ìš©ì–´ëŠ” ì •í™•íˆ ìœ ì§€í•˜ì„¸ìš”.

ì›ë³¸ í…ìŠ¤íŠ¸:
{cached_text}

êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´:"""

    try:
        payload = {
            "model": model_name,
            "prompt": formatting_prompt,
            "stream": False,
            "options": {
                "num_predict": 2048,
                "temperature": 0.3,  # Low temperature for consistent formatting
                "top_p": 0.9,
                "repeat_penalty": 1.1
            }
        }
        
        logger.debug(f"Sending cached text formatting request to Ollama: {model_name}")
        
        response = requests.post(
            settings.OLLAMA_API_URL, 
            json=payload, 
            timeout=settings.OLLAMA_TIMEOUT
        )
        
        if response.status_code == 200:
            result = response.json()
            formatted_text = result.get('response', '').strip()
            
            if formatted_text:
                logger.info(f"Successfully formatted cached response ({len(cached_text)} -> {len(formatted_text)} chars)")
                return formatted_text
            else:
                logger.warning("LLM returned empty formatted text, using original")
                return cached_text
        else:
            logger.error(f"Ollama formatting request failed: {response.status_code} - {response.text}")
            return cached_text
            
    except Exception as e:
        logger.error(f"Error formatting cached response with LLM: {e}")
        return cached_text

def get_llm_response(prompt: str, model_name: str = None, options: Dict = None, stream: bool = False) -> str | Generator[str, None, None]:
    """
    Sends a prompt to the Ollama LLM and gets a response.

    Args:
        prompt (str): The prompt to send to the LLM.
        model_name (str, optional): The name of the Ollama model to use.
                                    Defaults to DEFAULT_MODEL if not provided.
        options (Dict, optional): Additional Ollama model parameters (e.g., temperature).
                                  Refer to Ollama API documentation for available options.
        stream (bool, optional): Whether to stream the response. Defaults to False.
                                 If True, the function will yield chunks of the response.

    Returns:
        str: The LLM's response text if stream is False.
        Generator[str, None, None]: A generator that yields chunks of the LLM's response if stream is True.
    """
    if not model_name:
        model_name = settings.OLLAMA_DEFAULT_MODEL

    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": stream,
    }
    if options:
        payload["options"] = options

    logger.info(f"Sending prompt to Ollama model '{model_name}'. Prompt length: {len(prompt)} chars")
    logger.debug(f"Prompt preview: {prompt[:200]}...")

    try:
        response = requests.post(
            settings.OLLAMA_API_URL, 
            json=payload, 
            stream=True, # Always stream from requests, handle aggregation/yielding based on 'stream' arg
            timeout=settings.OLLAMA_TIMEOUT
        )
        response.raise_for_status()

        if stream:
            def generate():
                for line in response.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8')
                        try:
                            json_chunk = json.loads(decoded_line)
                            if 'response' in json_chunk:
                                yield json_chunk['response']
                            if json_chunk.get('done', False):
                                break
                        except json.JSONDecodeError:
                            logger.warning(f"Non-JSON line in stream: {decoded_line}")
            logger.info("Streaming response from Ollama.")
            return generate()
        else:
            full_response = ""
            for line in response.iter_lines():
                if line:
                    decoded_line = line.decode('utf-8')
                    try:
                        json_chunk = json.loads(decoded_line)
                        if 'response' in json_chunk:
                            full_response += json_chunk['response']
                        if json_chunk.get('done', False):
                            break
                    except json.JSONDecodeError:
                        logger.warning(f"Non-JSON line in stream: {decoded_line}")
            logger.info("Streamed response received and aggregated")
            return full_response.strip()

    except requests.exceptions.ConnectionError as e:
        error_msg = f"Could not connect to Ollama at {settings.OLLAMA_API_URL}. Ensure Ollama is running."
        logger.error(f"{error_msg} Details: {e}")
        raise LLMError(error_msg, "CONNECTION_ERROR")
    except requests.exceptions.HTTPError as e:
        error_msg = f"HTTP error occurred: {e}"
        logger.error(f"{error_msg}. Response: {e.response.text if e.response else 'No response body'}")
        raise LLMError(error_msg, "HTTP_ERROR")
    except requests.exceptions.Timeout as e:
        error_msg = f"Request timed out after {settings.OLLAMA_TIMEOUT} seconds"
        logger.error(f"{error_msg}: {e}")
        raise LLMError(error_msg, "TIMEOUT_ERROR")
    except Exception as e:
        error_msg = f"An unexpected error occurred: {e}"
        logger.error(error_msg)
        raise LLMError(error_msg, "UNEXPECTED_ERROR")

def construct_rag_prompt(query: str, context_chunks: List[str], lang: str = "ko") -> str:
    """
    Constructs a prompt for the LLM using the RAG pattern, tailored for Korean or English.

    Args:
        query (str): The user's original question.
        context_chunks (List[str]): A list of relevant text chunks retrieved from the vector database.
        lang (str, optional): Language of the prompt. "ko" for Korean, "en" for English. Defaults to "ko".

    Returns:
        str: The fully constructed prompt to be sent to the LLM.
    """
    context = "\n\n---\n\n".join(context_chunks) # ê° ì²­í¬ë¥¼ ëª…í™•íˆ êµ¬ë¶„

    if lang == "ko":
        prompt = f"""ì£¼ì–´ì§„ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

[ë¬¸ë§¥ ì •ë³´]
{context}
---

[ì§ˆë¬¸]
{query}

[ë‹µë³€ ì‘ì„± ì§€ì¹¨]
- ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì œëª©(#, ##), ëª©ë¡(-), ê°•ì¡°(**), í‘œ ë“±ì„ í™œìš©í•˜ì„¸ìš”.
- **ì£¼ì¡° ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”**: ì£¼í˜•, íƒ•êµ¬, ë¼ì´ì €, ì½”ì–´, íŒ¨í„´, ê²°í•¨, ì‚¬í˜•, ê¸ˆí˜• ë“±ì˜ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
- ë‹µë³€ êµ¬ì¡°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš” (ê°œìš” â†’ ìƒì„¸ì„¤ëª… â†’ ìš”ì•½).
- ì˜ˆì‹œë‚˜ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ê°€ ìˆë‹¤ë©´ í¬í•¨í•´ì£¼ì„¸ìš”.
- **ì¶œì²˜ í‘œì‹œëŠ” ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ê³ ë¬¸í—Œ í˜•íƒœë¡œ í•œ ë²ˆë§Œ í‘œì‹œí•˜ì„¸ìš”**.

[ì¶œì²˜ í‘œì‹œ ê·œì¹™]
- ë‹µë³€ ë³¸ë¬¸ì—ëŠ” ì¶œì²˜ë¥¼ ë°˜ë³µ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”.
- ë‹µë³€ ë§ˆì§€ë§‰ì— "## ğŸ“š ì°¸ê³ ë¬¸í—Œ" ì„¹ì…˜ì„ ë§Œë“¤ì–´ ì¶œì²˜ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.
- í˜•ì‹: "- ğŸ“„ ë¬¸ì„œID: [ê´€ë ¨ë‚´ìš© ìš”ì•½]"

[í’ˆì§ˆ ê¸°ì¤€]
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
- ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ êµ¬ì„±
- ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ
- í•œêµ­ì–´ë¡œ ì‘ì„±

ë‹µë³€:
"""
    else: # Default to English prompt
        prompt = f"""Based on the provided context information, please answer the following question.

[Context Information]
{context}
---

[Question]
{query}

[Instructions]
- Write your answer in Markdown format. Use tables, code, emphasis, lists, links, images, etc. as appropriate.
- Each context chunk has a [Source:DocumentID] tag. Be sure to include this information in your answer to clearly indicate the source.
- If you can find the answer to the question in the context information, please use that information to generate your answer.
- Your answer should be a complete sentence, clear, and as detailed and long as possible.
- If possible, include examples, tables, code, or additional explanations.
- If you cannot find the answer to the question in the context information, please respond with "It is difficult to answer the question based on the provided context information alone."
- Do not add personal opinions or information not present in the context.
- Please answer in English.

Answer:
"""
    return prompt

def construct_multimodal_rag_prompt(query: str, 
                                   context_chunks: List[str], 
                                   image_descriptions: List[str] = None,
                                   table_contents: List[str] = None,
                                   lang: str = "ko") -> str:
    """
    Constructs a multimodal prompt for the LLM using the RAG pattern.
    """
    if image_descriptions is None:
        image_descriptions = []
    if table_contents is None:
        table_contents = []
    
    # Prepare different content sections
    content_sections = []
    
    if context_chunks:
        text_context = "\n\n---\n\n".join(context_chunks)
        content_sections.append(f"í…ìŠ¤íŠ¸ ì •ë³´:\n{text_context}")
    
    # Image context temporarily disabled
    # if image_descriptions:
    #     image_context = "\n\n---\n\n".join(image_descriptions)
    #     content_sections.append(f"ì´ë¯¸ì§€ ì •ë³´:\n{image_context}")
    
    if table_contents:
        table_context = "\n\n---\n\n".join(table_contents)
        content_sections.append(f"í‘œ ì •ë³´:\n{table_context}")
    
    all_content = "\n\n=== === ===\n\n".join(content_sections)
    
    if lang == "ko":
        prompt = f"""ì£¼ì–´ì§„ ë©€í‹°ëª¨ë‹¬ ì»¨í…íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

{all_content}
---

[ì§ˆë¬¸]
{query}

[ë‹µë³€ ì‘ì„± ì§€ì¹¨]
- ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì œëª©(#, ##), ëª©ë¡(-), ê°•ì¡°(**), í‘œ ë“±ì„ í™œìš©í•˜ì„¸ìš”.
- **ì£¼ì¡° ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”**: ì£¼í˜•, íƒ•êµ¬, ë¼ì´ì €, ì½”ì–´, íŒ¨í„´, ê²°í•¨, ì‚¬í˜•, ê¸ˆí˜•, ì¿ í´ë¼, ìš©í•´, ì‘ê³  ë“±ì˜ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
- ë‹µë³€ êµ¬ì¡°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš” (ê°œìš” â†’ ìƒì„¸ì„¤ëª… â†’ ìš”ì•½).
- í‘œë¥¼ ì°¸ì¡°í•  ë•ŒëŠ” [í‘œ1], [í‘œ2] í˜•ì‹ì„ ì‚¬ìš©í•˜ê³ , ì¤‘ìš”í•œ ìˆ˜ì¹˜ë‚˜ ê²½í–¥ì„ ë¶„ì„í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- **ì¶œì²˜ í‘œì‹œëŠ” ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ê³ ë¬¸í—Œ í˜•íƒœë¡œ í•œ ë²ˆë§Œ í‘œì‹œí•˜ì„¸ìš”**.

[ì¶œì²˜ í‘œì‹œ ê·œì¹™]
- ë‹µë³€ ë³¸ë¬¸ì—ëŠ” ì¶œì²˜ë¥¼ ë°˜ë³µ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”.
- ë‹µë³€ ë§ˆì§€ë§‰ì— "## ğŸ“š ì°¸ê³ ë¬¸í—Œ" ì„¹ì…˜ì„ ë§Œë“¤ì–´ ì¶œì²˜ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.
- í…ìŠ¤íŠ¸ ì¶œì²˜: "- ğŸ“„ ë¬¸ì„œID: [ê´€ë ¨ë‚´ìš© ìš”ì•½]"
- í‘œ ì¶œì²˜: "- ğŸ“Š í‘œN (ë¬¸ì„œID): [í‘œ ë‚´ìš© ìš”ì•½]"

[í’ˆì§ˆ ê¸°ì¤€]
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
- ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ êµ¬ì„±
- ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ
- í•œêµ­ì–´ë¡œ ì‘ì„±

ë‹µë³€:
"""
    else:
        prompt = f"""Please answer the following question based on the given multimodal content information.

{all_content}
---

[Question]
{query}

[Instructions]
- Write your answer in Markdown format. Use tables, code, emphasis, lists, links, etc. as appropriate.
- Each content piece has a [Source:DocumentID] tag. Be sure to include this information in your answer to clearly indicate the source.
- Integrate text, image, and table information to provide a comprehensive answer to the question.
- If images or tables are available, reference their content in your explanation using phrases like "As shown in the image above..." or "According to the data in the table..."
- Your answer should be a complete sentence, clear, and as detailed and long as possible.
- If possible, include examples, tables, code, or additional explanations.
- If you cannot find the answer to the question in the provided content, please respond with "It is difficult to answer the question based on the provided content information alone."
- Do not add personal opinions or information not present in the content.
- If there are charts or graphs in images, describe their content specifically.
- If there is table data, analyze and explain important figures or trends.
- Please answer in English.

Answer:
"""
    
    return prompt

def enhance_response_with_media_references(response: str, 
                                         images: List[Dict[str, Any]], 
                                         tables: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Enhances the LLM response with references to images and tables that can be displayed in the UI.
    
    Returns:
        Dict containing enhanced response with media references
    """
    enhanced_response = {
        "text": response,
        "referenced_images": [],
        "referenced_tables": [],
        "has_media": False
    }
    
    # Image references temporarily disabled
    # for idx, img in enumerate(images, 1):
    #     meta = img.get('metadata', {})
    #     enhanced_response["referenced_images"].append({
    #         "index": idx,
    #         "filename": meta.get('filename', ''),
    #         "path": meta.get('file_path', ''),
    #         "page": meta.get('page', ''),
    #         "source": meta.get('source_document_id', ''),
    #         "description": img.get('description', '')
    #     })
    
    # Add all available tables with index numbers for UI display
    for idx, table in enumerate(tables, 1):
        meta = table.get('metadata', {})
        enhanced_response["referenced_tables"].append({
            "index": idx,
            "filename": meta.get('filename', ''),
            "path": meta.get('file_path', ''),
            "page": meta.get('page', ''),
            "source": meta.get('source_document_id', ''),
            "content": table.get('content', ''),
            "parsed_data": table.get('parsed_data', [])
        })
    
    enhanced_response["has_media"] = bool(
        enhanced_response["referenced_tables"]
    )
    
    return enhanced_response

# ë¹„ë™ê¸° LLM ì²˜ë¦¬ í•¨ìˆ˜ë“¤
async def get_llm_response_async(
    prompt: str, 
    model_name: str = None, 
    options: Dict[str, Any] = None, 
    stream: bool = False
):
    """
    ë¹„ë™ê¸° LLM ì‘ë‹µ ìƒì„±
    """
    if model_name is None:
        model_name = settings.OLLAMA_DEFAULT_MODEL

    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": stream,
    }
    if options:
        payload["options"] = options

    logger.info(f"Sending async prompt to Ollama model '{model_name}'. Prompt length: {len(prompt)} chars")

    try:
        timeout = aiohttp.ClientTimeout(total=settings.OLLAMA_TIMEOUT)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(settings.OLLAMA_API_URL, json=payload) as response:
                response.raise_for_status()
                
                if stream:
                    async def generate():
                        async for line in response.content:
                            if line:
                                try:
                                    decoded_line = line.decode('utf-8').strip()
                                    if decoded_line:
                                        json_chunk = json.loads(decoded_line)
                                        if 'response' in json_chunk:
                                            yield json_chunk['response']
                                        if json_chunk.get('done', False):
                                            break
                                except json.JSONDecodeError:
                                    logger.warning(f"Non-JSON line in async stream: {decoded_line}")
                                    continue
                    return generate()
                else:
                    # ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                    full_response = ""
                    async for line in response.content:
                        if line:
                            try:
                                decoded_line = line.decode('utf-8').strip()
                                if decoded_line:
                                    json_chunk = json.loads(decoded_line)
                                    if 'response' in json_chunk:
                                        full_response += json_chunk['response']
                                    if json_chunk.get('done', False):
                                        break
                            except json.JSONDecodeError:
                                logger.warning(f"Non-JSON line in async response: {decoded_line}")
                                continue
                    return full_response

    except aiohttp.ClientConnectorError as e:
        error_msg = f"Could not connect to Ollama at {settings.OLLAMA_API_URL}. Ensure Ollama is running."
        logger.error(f"{error_msg} Details: {e}")
        raise LLMError(error_msg, "CONNECTION_ERROR")
    except aiohttp.ClientResponseError as e:
        error_msg = f"HTTP error occurred: {e}"
        logger.error(f"{error_msg}. Status: {e.status}")
        raise LLMError(error_msg, "HTTP_ERROR")
    except asyncio.TimeoutError as e:
        error_msg = f"Async request timed out after {settings.OLLAMA_TIMEOUT} seconds"
        logger.error(f"{error_msg}: {e}")
        raise LLMError(error_msg, "TIMEOUT_ERROR")
    except Exception as e:
        error_msg = f"An unexpected async error occurred: {e}"
        logger.error(error_msg)
        raise LLMError(error_msg, "UNEXPECTED_ERROR")

async def process_multimodal_rag_async(
    query: str, 
    search_results: Dict[str, List[Dict[str, Any]]], 
    model_name: str = None, 
    lang: str = "ko"
) -> Dict[str, Any]:
    """
    ë¹„ë™ê¸° ë©€í‹°ëª¨ë‹¬ RAG ì²˜ë¦¬
    """
    # ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
    text_chunks = [result['document'] for result in search_results.get('text', [])]
    compressed_context = compress_context_for_performance(text_chunks, max_tokens=2000)
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = construct_rag_prompt(query, compressed_context, lang)
    
    # ë¹„ë™ê¸° LLM í˜¸ì¶œ
    response = await get_llm_response_async(prompt, model_name)
    
    # ì‘ë‹µ ê°•í™”
    enhanced_response = enhance_response_with_metadata(
        response, search_results.get('text', []), [], search_results.get('tables', [])
    )
    
    return enhanced_response

def compress_context_for_performance(text_chunks: List[str], max_tokens: int = 2000) -> List[str]:
    """
    ì„±ëŠ¥ì„ ìœ„í•œ ì˜ë¯¸ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
    """
    if not text_chunks:
        return []
    
    # í† í° ìˆ˜ ì¶”ì • (1 í† í° â‰ˆ 4 ê¸€ì)
    total_chars = sum(len(chunk) for chunk in text_chunks)
    estimated_tokens = total_chars // 4
    
    if estimated_tokens <= max_tokens:
        return text_chunks
    
    logger.info(f"Context compression needed: {estimated_tokens} -> {max_tokens} tokens")
    
    # ì˜ë¯¸ ê¸°ë°˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
    scored_chunks = []
    for i, chunk in enumerate(text_chunks):
        score = _calculate_chunk_importance(chunk, i, len(text_chunks))
        scored_chunks.append((chunk, score))
    
    # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    scored_chunks.sort(key=lambda x: x[1], reverse=True)
    
    # í† í° ì œí•œ ë‚´ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì²­í¬ë“¤ ì„ íƒ
    compressed_chunks = []
    current_tokens = 0
    max_chars = max_tokens * 4
    
    for chunk, score in scored_chunks:
        chunk_tokens = len(chunk) // 4
        
        if current_tokens + chunk_tokens <= max_tokens:
            compressed_chunks.append(chunk)
            current_tokens += chunk_tokens
        else:
            # ë‚¨ì€ ê³µê°„ì´ ìˆìœ¼ë©´ ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ë¶€ë¶„ í¬í•¨
            remaining_chars = max_chars - (current_tokens * 4)
            if remaining_chars > 200:  # ìµœì†Œ 200ì ì´ìƒì¼ ë•Œë§Œ
                # ì²­í¬ì˜ ì²« ë¶€ë¶„ê³¼ ë§ˆì§€ë§‰ ë¶€ë¶„ í¬í•¨ (ìš”ì•½ íš¨ê³¼)
                half_remaining = remaining_chars // 2
                truncated = chunk[:half_remaining] + " ... " + chunk[-half_remaining:]
                compressed_chunks.append(truncated)
            break
    
    # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬ (ì‹œê°„ ìˆœì„œ ìœ ì§€)
    chunk_to_index = {chunk: i for i, chunk in enumerate(text_chunks)}
    compressed_chunks.sort(key=lambda x: _get_original_index(x, chunk_to_index))
    
    logger.info(f"Compressed context: {len(text_chunks)} -> {len(compressed_chunks)} chunks")
    return compressed_chunks

def _calculate_chunk_importance(chunk: str, position: int, total_chunks: int) -> float:
    """
    ì²­í¬ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
    """
    import re
    
    score = 0.0
    
    # 1. ì£¼ì¡° ì „ë¬¸ìš©ì–´ í¬í•¨ ì •ë„ (ê°€ì¤‘ì¹˜: 3.0)
    foundry_keywords = [
        'ì£¼í˜•', 'íƒ•êµ¬', 'ë¼ì´ì €', 'ì½”ì–´', 'íŒ¨í„´', 'ì‚¬í˜•', 'ê¸ˆí˜•', 'ê²°í•¨', 'ìˆ˜ì¶•', 'ì‘ê³ ',
        'ìš©í•´', 'ì¿ í´ë¼', 'ì¡°í˜•', 'ì£¼ì…', 'ìºìŠ¤íŒ…', 'ëª°ë“œ', 'ìŠ¤í”„ë£¨', 'ê²Œì´íŠ¸', 'ëŸ°ë„ˆ',
        'ë²¤íŠ¸', 'ì´í˜•ì œ', 'ë°”ì¸ë”', 'ê·œì‚¬', 'ì í† ', 'ë²¤í† ë‚˜ì´íŠ¸', 'ê¸°í¬', 'ê· ì—´', 'í•€í™€',
        'casting', 'mold', 'core', 'pattern', 'defect', 'shrinkage', 'porosity'
    ]
    
    keyword_count = sum(1 for keyword in foundry_keywords if keyword.lower() in chunk.lower())
    score += keyword_count * 3.0
    
    # 2. ìˆ«ì/ë°ì´í„° í¬í•¨ ì •ë„ (ê°€ì¤‘ì¹˜: 2.0)
    numbers = re.findall(r'\d+(?:\.\d+)?', chunk)
    score += len(numbers) * 0.5
    
    # 3. í‘œ ì°¸ì¡°ë‚˜ ì´ë¯¸ì§€ ì°¸ì¡° (ê°€ì¤‘ì¹˜: 2.5)
    table_refs = re.findall(r'í‘œ\s*\d+|Table\s*\d+|\[í‘œ\s*\d+\]', chunk, re.IGNORECASE)
    image_refs = re.findall(r'ê·¸ë¦¼\s*\d+|Figure\s*\d+|\[ê·¸ë¦¼\s*\d+\]', chunk, re.IGNORECASE)
    score += (len(table_refs) + len(image_refs)) * 2.5
    
    # 4. í…ìŠ¤íŠ¸ ê¸¸ì´ (ê°€ì¤‘ì¹˜: 1.0) - ë” ê¸´ í…ìŠ¤íŠ¸ëŠ” ë” ë§ì€ ì •ë³´ í¬í•¨
    score += len(chunk) / 1000.0
    
    # 5. ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ - ë¬¸ì„œ ì‹œì‘/ë ë¶€ë¶„ì€ ì¤‘ìš”í•  ê°€ëŠ¥ì„± ë†’ìŒ
    position_weight = 1.0
    if position < total_chunks * 0.2:  # ì• 20%
        position_weight = 1.2
    elif position > total_chunks * 0.8:  # ë’¤ 20%
        position_weight = 1.1
    
    score *= position_weight
    
    # 6. ë¬¸ì¥ ì™„ì„±ë„ (ê°€ì¤‘ì¹˜: 1.5)
    sentences = re.split(r'[.!?]', chunk)
    complete_sentences = [s for s in sentences if s.strip() and len(s.strip()) > 10]
    score += len(complete_sentences) * 1.5
    
    return score

def _get_original_index(chunk: str, chunk_to_index: dict) -> int:
    """
    ì›ë˜ ì²­í¬ ì¸ë±ìŠ¤ ì°¾ê¸° (ë¶€ë¶„ ì¼ì¹˜ ê³ ë ¤)
    """
    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
    if chunk in chunk_to_index:
        return chunk_to_index[chunk]
    
    # ë¶€ë¶„ ì¼ì¹˜í•˜ëŠ” ê²½ìš° (ì••ì¶•ëœ ì²­í¬)
    for original_chunk, index in chunk_to_index.items():
        if chunk.startswith(original_chunk[:100]):  # ì²« 100ìë¡œ ë§¤ì¹­
            return index
    
    return 999999  # ì°¾ì§€ ëª»í•œ ê²½ìš° ë§¨ ë’¤ë¡œ

def process_llm_chat_request(user_query: str,
                             retrieved_content: Dict[str, Any],
                             model_name: str = None,
                             lang: str = "ko",
                             stream: bool = False) -> Dict[str, Any] | Generator[Dict[str, Any], None, None]:
    """
    High-level function to process a chat request using RAG, supporting both text and multimodal content.
    1. Constructs a prompt from the user query and retrieved documents/multimodal content.
    2. Gets a response from the LLM.
    3. Enhances the response with media references.

    Args:
        user_query (str): The user's question.
        retrieved_content (Dict[str, Any]): Dict containing 'text_chunks', 'images', 'tables'.
                                            For text-only, only 'text_chunks' is needed.
        model_name (str, optional): Ollama model name. Defaults to settings.OLLAMA_DEFAULT_MODEL.
        lang (str, optional): Language for the prompt. Defaults to "ko".
        stream (bool, optional): Whether to stream the response. Defaults to False.

    Returns:
        Dict[str, Any]: The LLM's response, potentially with media references, if stream is False.
        Generator[Dict[str, Any], None, None]: A generator that yields chunks of the LLM's response,
                                                potentially with media references, if stream is True.
    """
    text_chunks = retrieved_content.get('text', [])
    images = retrieved_content.get('images', [])
    tables = retrieved_content.get('tables', [])

    if not any([text_chunks, images, tables]):
        if lang == "ko":
            return {"text": "ê´€ë ¨ ì»¨í…íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "referenced_images": [], "referenced_tables": [], "has_media": False}
        else:
            return {"text": "No relevant content found to generate an answer.", "referenced_images": [], "referenced_tables": [], "has_media": False}

    # Prepare context for prompt construction
    context_chunks_for_prompt = []
    for doc in text_chunks:
        text = doc.get("text", "")
        meta = doc.get("metadata", {})
        source = meta.get("source_document_id", "?")
        chunk_index = meta.get("chunk_index", "?")
        if text:
            context_chunks_for_prompt.append(f"[ì¶œì²˜:{source}_ì²­í¬{chunk_index}]\n{text}")

    # Determine if it's a multimodal request
    is_multimodal = bool(images or tables)

    if is_multimodal:
        prompt = construct_multimodal_rag_prompt(
            user_query, 
            context_chunks_for_prompt, 
            image_descriptions=[], # Image descriptions are not used in prompt for now
            table_contents=[f"[í‘œ{idx}] [ì¶œì²˜:{t.get('metadata', {}).get('source_document_id', '?')}_í˜ì´ì§€{t.get('metadata', {}).get('page', '?')}]\n{t.get('content', '')}" for idx, t in enumerate(tables, 1)],
            lang=lang
        )
        ollama_options = {
            "temperature": settings.LLM_TEMPERATURE,
            "num_predict": settings.LLM_NUM_PREDICT_MULTIMODAL,
        }
    else:
        prompt = construct_rag_prompt(user_query, context_chunks_for_prompt, lang=lang)
        ollama_options = {
            "temperature": settings.LLM_TEMPERATURE,
            "num_predict": settings.LLM_NUM_PREDICT_TEXT,
        }

    try:
        if stream:
            def streaming_response_generator():
                first_chunk = True
                for chunk_text in get_llm_response(prompt, model_name=model_name, options=ollama_options, stream=True):
                    if first_chunk:
                        # Send initial metadata (referenced images/tables) with the first text chunk
                        initial_response = enhance_response_with_media_references("", images, tables)
                        initial_response["text"] = chunk_text # Add first text chunk
                        yield initial_response
                        first_chunk = False
                    else:
                        yield {"text": chunk_text, "referenced_images": [], "referenced_tables": [], "has_media": False} # Subsequent chunks only contain text
            return streaming_response_generator()
        else:
            llm_response_text = get_llm_response(prompt, model_name=model_name, options=ollama_options, stream=False)
            logger.info(f"Successfully generated LLM response for query: {user_query[:50]}...")
            
            # Enhance response with media references if multimodal
            return enhance_response_with_media_references(llm_response_text, images, tables)

    except LLMError as e:
        logger.error(f"LLM error in chat request: {e}")
        if lang == "ko":
            return {"text": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e.message}", "referenced_images": [], "referenced_tables": [], "has_media": False}
        else:
            return {"text": f"An error occurred while generating response: {e.message}", "referenced_images": [], "referenced_tables": [], "has_media": False}


if __name__ == '__main__':
    print(f"LLM service module loaded. Ollama API URL: {settings.OLLAMA_API_URL}, Default Model: {settings.OLLAMA_DEFAULT_MODEL}")

    # --- Test RAG Prompt Construction ---
    print("\n--- Testing RAG Prompt Construction (Korean) ---")
    sample_query_ko = "ì½”ì½”ë„›ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    sample_context_ko = [
        "ì½”ì½”ë„›ì€ ë‹¤ì–‘í•œ ê±´ê°• íš¨ëŠ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì½”ì½”ë„› ì˜¤ì¼ì€ í”¼ë¶€ ë³´ìŠµì— ì¢‹ìŠµë‹ˆë‹¤.",
        "ë˜í•œ, ì½”ì½”ë„› ì›Œí„°ëŠ” ì „í•´ì§ˆì´ í’ë¶€í•˜ì—¬ ìš´ë™ í›„ ìˆ˜ë¶„ ë³´ì¶©ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    ]
    korean_prompt = construct_rag_prompt(sample_query_ko, sample_context_ko, lang="ko")
    print(f"Generated Korean RAG Prompt:\n{korean_prompt}")

    # --- Test Multimodal RAG Prompt Construction ---
    print("\n--- Testing Multimodal RAG Prompt Construction (Korean) ---")
    sample_multimodal_query_ko = "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì´ë©°, í‘œì—ëŠ” ì–´ë–¤ ë°ì´í„°ê°€ ìˆë‚˜ìš”?"
    sample_multimodal_text_chunks = [
        {"text": "ì£¼ì¡° ê³µì •ì€ ê¸ˆì†ì„ ë…¹ì—¬ ê±°í‘¸ì§‘ì— ë¶€ì–´ ì œí’ˆì„ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤.", "metadata": {"source_document_id": "doc1", "chunk_index": 0}},
        {"text": "ì£¼ìš” ê²°í•¨ìœ¼ë¡œëŠ” ê¸°í¬, ìˆ˜ì¶•, ê· ì—´ ë“±ì´ ìˆìŠµë‹ˆë‹¤.", "metadata": {"source_document_id": "doc1", "chunk_index": 1}}
    ]
    sample_multimodal_images = [] # Temporarily disabled
    sample_multimodal_tables = [
        {"content": "ì˜¨ë„ | ê°•ë„\n1000C | 200MPa\n1200C | 150MPa", "metadata": {"source_document_id": "doc1", "page": 5}, "parsed_data": [["ì˜¨ë„", "ê°•ë„"], ["1000C", "200MPa"], ["1200C", "150MPa"]]}
    ]
    
    multimodal_content_for_test = {
        "text": sample_multimodal_text_chunks,
        "images": sample_multimodal_images,
        "tables": sample_multimodal_tables
    }

    print("\n--- Testing process_llm_chat_request (non-streaming) ---")
    multimodal_response_non_stream = process_llm_chat_request(
        sample_multimodal_query_ko,
        multimodal_content_for_test,
        model_name=settings.OLLAMA_DEFAULT_MODEL,
        lang="ko",
        stream=False
    )
    print(f"Processed LLM Chat Response (non-streaming):\n{multimodal_response_non_stream['text']}")
    if multimodal_response_non_stream['referenced_tables']:
        print(f"Referenced Tables: {multimodal_response_non_stream['referenced_tables']}")

    print("\n--- Testing process_llm_chat_request (streaming) ---")
    print("Streaming response:")
    multimodal_response_stream = process_llm_chat_request(
        sample_multimodal_query_ko,
        multimodal_content_for_test,
        model_name=settings.OLLAMA_DEFAULT_MODEL,
        lang="ko",
        stream=True
    )
    full_streamed_response = ""
    for chunk in multimodal_response_stream:
        print(chunk.get("text", ""), end='')
        full_streamed_response += chunk.get("text", "")
        if chunk.get("referenced_tables"):
            print(f"\n(Referenced Tables in stream: {chunk['referenced_tables']})\n")
    print(f"\nFull streamed response length: {len(full_streamed_response)}")

    print("\nTo run LLM call tests, ensure your Ollama server is running and the model is available.")
